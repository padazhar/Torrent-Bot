{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGJ6IPSR3IIJxx5DSwpvdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/padazhar/Torrent-Bot/blob/main/Colab/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxDnu3ovEBFH"
      },
      "outputs": [],
      "source": [
        "import py1337x\n",
        "import telebot\n",
        "import requests\n",
        "import requests_cache\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def torrentParser(response, baseUrl, page=1):\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    torrentList = soup.select('a[href*=\"/torrent/\"]')\n",
        "    seedersList = soup.select('td.coll-2')\n",
        "    leechersList = soup.select('td.coll-3')\n",
        "    sizeList = soup.select('td.coll-4')\n",
        "    timeList = soup.select('td.coll-date')\n",
        "    uploaderList = soup.select('td.coll-5')\n",
        "\n",
        "    lastPage = soup.find('div', {'class': 'pagination'})\n",
        "\n",
        "    if not lastPage:\n",
        "        pageCount = page\n",
        "    else:\n",
        "        try:\n",
        "            pageCount = int(lastPage.findAll('a')[-1]['href'].split('/')[-2])\n",
        "\n",
        "        except Exception:\n",
        "            pageCount = page\n",
        "\n",
        "    results = {\n",
        "        'items': [],\n",
        "        'currentPage': page or 1,\n",
        "        'itemCount': len(torrentList),\n",
        "        'pageCount': pageCount\n",
        "    }\n",
        "\n",
        "    if torrentList:\n",
        "        for count, torrent in enumerate(torrentList):\n",
        "            name = torrent.getText().strip()\n",
        "            torrentId = torrent['href'].split('/')[2]\n",
        "            link = baseUrl+torrent['href']\n",
        "            seeders = seedersList[count].getText()\n",
        "            leechers = leechersList[count].getText()\n",
        "            size = sizeList[count].contents[0]\n",
        "            time = timeList[count].getText()\n",
        "            uploader = uploaderList[count].getText().strip()\n",
        "            uploaderLink = baseUrl+'/'+uploader+'/'\n",
        "\n",
        "            results['items'].append({\n",
        "                'name': name,\n",
        "                'torrentId': torrentId,\n",
        "                'link': link,\n",
        "                'seeders': seeders,\n",
        "                'leechers': leechers,\n",
        "                'size': size,\n",
        "                'time': time,\n",
        "                'uploader': uploader,\n",
        "                'uploaderLink': uploaderLink\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def infoParser(response, baseUrl):\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    name = soup.find('div', {'class': 'box-info-heading clearfix'})\n",
        "    name = name.text.strip() if name else None\n",
        "\n",
        "    shortName = soup.find('div', {'class': 'torrent-detail-info'})\n",
        "    shortName = shortName.find('h3').getText().strip() if shortName else None\n",
        "\n",
        "    description = soup.find('div', {'class': 'torrent-detail-info'})\n",
        "    description = description.find('p').getText().strip() if description else None\n",
        "\n",
        "    genre = soup.find('div', {'class': 'torrent-category clearfix'})\n",
        "    genre = [i.text.strip() for i in genre.find_all('span')] if genre else None\n",
        "\n",
        "    thumbnail = soup.find('div', {'class': 'torrent-image'})\n",
        "    thumbnail = thumbnail.find('img')['src'] if thumbnail else None\n",
        "\n",
        "    if thumbnail and not thumbnail.startswith('http'):\n",
        "        if thumbnail.startswith('//'):\n",
        "            thumbnail = 'https:'+thumbnail\n",
        "        else:\n",
        "            thumbnail = baseUrl+thumbnail\n",
        "\n",
        "    magnetLink = soup.select('a[href^=\"magnet\"]')\n",
        "    magnetLink = magnetLink[0]['href'] if magnetLink else None\n",
        "\n",
        "    infoHash = soup.find('div', {'class': 'infohash-box'})\n",
        "    infoHash = infoHash.find('span').getText() if infoHash else None\n",
        "\n",
        "    images = soup.find('div', {'class': 'tab-pane active'})\n",
        "    images = [i['src'] for i in images.find_all('img')] if images else None\n",
        "\n",
        "    descriptionList = soup.find_all('ul', {'class': 'list'})\n",
        "\n",
        "    if len(descriptionList) > 2:\n",
        "        firstList = descriptionList[1].find_all('li')\n",
        "        secondList = descriptionList[2].find_all('li')\n",
        "\n",
        "        category = firstList[0].find('span').getText()\n",
        "        species = firstList[1].find('span').getText()\n",
        "        language = firstList[2].find('span').getText()\n",
        "        size = firstList[3].find('span').getText()\n",
        "        uploader = firstList[4].find('span').getText().strip()\n",
        "        uploaderLink = baseUrl+'/'+uploader+'/'\n",
        "\n",
        "        downloads = secondList[0].find('span').getText()\n",
        "        lastChecked = secondList[1].find('span').getText()\n",
        "        uploadDate = secondList[2].find('span').getText()\n",
        "        seeders = secondList[3].find('span').getText()\n",
        "        leechers = secondList[4].find('span').getText()\n",
        "\n",
        "    else:\n",
        "        category = species = language = size = uploader = uploaderLink = downloads = lastChecked = uploadDate = seeders = leechers = None\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'shortName': shortName,\n",
        "        'description': description,\n",
        "        'category': category,\n",
        "        'type': species,\n",
        "        'genre': genre,\n",
        "        'language': language,\n",
        "        'size': size,\n",
        "        'thumbnail': thumbnail,\n",
        "        'images': images if images else None,\n",
        "        'uploader': uploader,\n",
        "        'uploaderLink': uploaderLink,\n",
        "        'downloads': downloads,\n",
        "        'lastChecked': lastChecked,\n",
        "        'uploadDate': uploadDate,\n",
        "        'seeders': seeders,\n",
        "        'leechers': leechers,\n",
        "        'magnetLink': magnetLink,\n",
        "        'infoHash': infoHash.strip() if infoHash else None\n",
        "    }\n",
        "\n",
        "\n",
        "class py1337x():\n",
        "    def __init__(self, proxy=None, cookie=None, cache=None, cacheTime=86400, backend='sqlite'):\n",
        "        self.baseUrl = f'https://www.{proxy}' if proxy else 'https://www.1377x.to'\n",
        "        self.headers = {\n",
        "            'user-agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
        "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'accept-language': 'en-US,en;q=0.5',\n",
        "            'upgrade-insecure-requests': '1',\n",
        "            'te': 'trailers'\n",
        "        }\n",
        "\n",
        "        if cookie:\n",
        "            self.headers['cookie'] = f'cf_clearance={cookie}'\n",
        "\n",
        "        self.requests = requests_cache.CachedSession(cache, expire_after=cacheTime, backend=backend) if cache else requests\n",
        "\n",
        "    #: Searching torrents\n",
        "    def search(self, query, page=1, category=None, sortBy=None, order='desc'):\n",
        "        query = '+'.join(query.split())\n",
        "        category = category.upper() if category and category.lower() in ['xxx', 'tv'] else category.capitalize() if category else None\n",
        "        url = f\"{self.baseUrl}/{'sort-' if sortBy else ''}{'category-' if category else ''}search/{query}/{category+'/' if category else ''}{sortBy.lower()+'/' if sortBy else ''}{order.lower()+'/' if sortBy else ''}{page}/\"\n",
        "\n",
        "        response = self.requests.get(url, headers=self.headers)\n",
        "        return torrentParser(response, baseUrl=self.baseUrl, page=page)\n",
        "\n",
        "    #: Trending torrents\n",
        "    def trending(self, category=None, week=False):\n",
        "        url = f\"{self.baseUrl}/trending{'-week' if week and not category else ''}{'/w/'+category.lower()+'/' if week and category else '/d/'+category.lower()+'/' if not week and category else ''}\"\n",
        "\n",
        "        response = self.requests.get(url, headers=self.headers)\n",
        "        return torrentParser(response, baseUrl=self.baseUrl)\n",
        "\n",
        "    #: Top 100 torrents\n",
        "    def top(self, category=None):\n",
        "        category = 'applications' if category and category.lower() == 'apps' else 'television' if category and category.lower() == 'tv' else category.lower() if category else None\n",
        "        url = f\"{self.baseUrl}/top-100{'-'+category if category else ''}\"\n",
        "\n",
        "        response = self.requests.get(url, headers=self.headers)\n",
        "        return torrentParser(response, baseUrl=self.baseUrl)\n",
        "\n",
        "    #: Popular torrents\n",
        "    def popular(self, category, week=False):\n",
        "        url = f\"{self.baseUrl}/popular-{category.lower()}{'-week' if week else ''}\"\n",
        "\n",
        "        response = self.requests.get(url, headers=self.headers)\n",
        "        return torrentParser(response, baseUrl=self.baseUrl)\n",
        "\n",
        "    #: Browse torrents by category type\n",
        "    def browse(self, category, page=1):\n",
        "        category = category.upper() if category.lower() in ['xxx', 'tv'] else category.capitalize()\n",
        "        url = f'{self.baseUrl}/cat/{category}/{page}/'\n",
        "\n",
        "        response = self.requests.get(url, headers=self.headers)\n",
        "        return torrentParser(response, baseUrl=self.baseUrl, page=page)\n",
        "\n",
        "    #: Info of torrent\n",
        "    def info(self, link=None, torrentId=None):\n",
        "        if not link and not torrentId:\n",
        "            raise TypeError('Missing 1 required positional argument: link or torrentId')\n",
        "        elif link and torrentId:\n",
        "            raise TypeError('Got an unexpected argument: Pass either link or torrentId')\n",
        "\n",
        "        link = f'{self.baseUrl}/torrent/{torrentId}/h9/' if torrentId else link\n",
        "        response = self.requests.get(link, headers=self.headers)\n",
        "\n",
        "        return infoParser(response, baseUrl=self.baseUrl)"
      ],
      "metadata": {
        "id": "VfmAJWcgGMwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}